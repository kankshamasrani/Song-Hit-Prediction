{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print \"Hello World\"\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams, trigrams\n",
    "import math\n",
    "import sklearn\n",
    "import collections \n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(\"[\\wâ€™]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nb', 'nest', 'list', 'comprehensions', 'inside', 'could', 'write', 'example', 'single', 'statement', 'without', 'need', 'temporary', 'variable', 'noprimes', 'however', 'lines', 'tend', 'get', 'long', 'less', 'readable', 'recommended']\n"
     ]
    }
   ],
   "source": [
    "s= '''NB: You can nest list comprehensions inside of each other, so you could write the above example with a single statement (without the need for the temporary variable \"noprimes\"). However, such lines tend to get long and less readable, so this is not recommended.'''\n",
    "a = tokenizer.tokenize(s.lower())\n",
    "l = [t for t in a if t not in stopwords]\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see 0.0\n",
      "hey 0.0\n",
      "like 0.0\n",
      "always 0.0\n",
      "[('queen', 0.048114406802982915), ('really', 0.048114406802982915), ('thinks', 0.040095339002485755), ('meet', 0.040095339002485755), ('well', 0.0320762712019886), ('live', 0.0320762712019886), ('virginia', 0.0320762712019886), ('wants', 0.0320762712019886), ('pulls', 0.024057203401491457), ('screams', 0.024057203401491457), ('wanna', 0.017757595975540046), ('beautiful', 0.0160381356009943), ('doesn', 0.0160381356009943), ('unusual', 0.0160381356009943), ('president', 0.0160381356009943), ('scene', 0.0160381356009943), ('wait', 0.0160381356009943), ('hair', 0.014797996646283371), ('life', 0.014797996646283371), ('back', 0.008878797987770023), ('carborators', 0.00801906780049715), ('intuition', 0.00801906780049715), ('mediator', 0.00801906780049715), ('midnight', 0.00801906780049715), ('anyway', 0.00801906780049715), ('exercises', 0.00801906780049715), ('virgina', 0.00801906780049715), ('mmmm', 0.00801906780049715), ('home', 0.00801906780049715), ('heels', 0.00801906780049715), ('dress', 0.00801906780049715), ('sit', 0.00801906780049715), ('surprises', 0.00801906780049715), ('confess', 0.00801906780049715), ('stealin', 0.00801906780049715), ('shape', 0.00801906780049715), ('won', 0.00801906780049715), ('tragic', 0.00801906780049715), ('babies', 0.00801906780049715), ('mama', 0.00801906780049715), ('drinks', 0.00801906780049715), ('hates', 0.00801906780049715), ('body', 0.00801906780049715), ('beautuiful', 0.00801906780049715), ('mess', 0.00801906780049715), ('comprimises', 0.00801906780049715), ('moment', 0.00801906780049715), ('wrestles', 0.00801906780049715), ('coffee', 0.00801906780049715), ('rip', 0.00801906780049715), ('timing', 0.00801906780049715), ('alligators', 0.00801906780049715), ('quite', 0.00801906780049715), ('wears', 0.00801906780049715), ('care', 0.00801906780049715), ('high', 0.00801906780049715), ('works', 0.00801906780049715), ('magic', 0.00801906780049715), ('smokes', 0.00801906780049715), ('brother', 0.00801906780049715), ('alone', 0.00801906780049715), ('loves', 0.00801906780049715), ('fine', 0.00801906780049715), ('catch', 0.00801906780049715), ('daddy', 0.00801906780049715), ('confidence', 0.00801906780049715), ('pack', 0.00801906780049715), ('yeah', 0.0059191986585133485), ('right', 0.0029595993292566743), ('never', 0.0029595993292566743), ('phone', 0.0029595993292566743), ('day', 0.0029595993292566743), ('thing', 0.0029595993292566743), ('ain', 0.0029595993292566743), ('see', 0.0), ('hey', 0.0), ('like', 0.0), ('always', 0.0)]\n",
      "hey 0.0\n",
      "like 0.0\n",
      "always 0.0\n",
      "see 0.0\n",
      "[('sister', 0.048366578746394766), ('tonight', 0.0414570674969098), ('single', 0.03454755624742484), ('fair', 0.0207285337484549), ('move', 0.0207285337484549), ('radio', 0.0207285337484549), ('mister', 0.0207285337484549), ('stereo', 0.0207285337484549), ('soul', 0.017850665136837425), ('ain', 0.015300570117289221), ('knew', 0.013819022498969935), ('dream', 0.013819022498969935), ('wanna', 0.012750475097741019), ('one', 0.010200380078192815), ('miss', 0.010200380078192815), ('way', 0.010200380078192815), ('thing', 0.010200380078192815), ('mind', 0.007650285058644611), ('know', 0.007650285058644611), ('heart', 0.0069095112494849675), ('forget', 0.0069095112494849675), ('show', 0.0069095112494849675), ('stains', 0.0069095112494849675), ('sweet', 0.0069095112494849675), ('direction', 0.0069095112494849675), ('brain', 0.0069095112494849675), ('decided', 0.0069095112494849675), ('want', 0.0069095112494849675), ('dreaming', 0.0069095112494849675), ('need', 0.0069095112494849675), ('madonna', 0.0069095112494849675), ('cut', 0.0069095112494849675), ('lipstick', 0.0069095112494849675), ('track', 0.0069095112494849675), ('moonbeam', 0.0069095112494849675), ('virgin', 0.0069095112494849675), ('smell', 0.0069095112494849675), ('gave', 0.0069095112494849675), ('wouldn', 0.0069095112494849675), ('obsessed', 0.0069095112494849675), ('watching', 0.0069095112494849675), ('beat', 0.0069095112494849675), ('gonna', 0.0069095112494849675), ('bound', 0.0069095112494849675), ('game', 0.0069095112494849675), ('part', 0.0069095112494849675), ('let', 0.0069095112494849675), ('front', 0.0069095112494849675), ('world', 0.0069095112494849675), ('believe', 0.0069095112494849675), ('glad', 0.0069095112494849675), ('every', 0.0069095112494849675), ('deny', 0.0069095112494849675), ('kind', 0.0069095112494849675), ('gangster', 0.0069095112494849675), ('untrimmed', 0.0069095112494849675), ('thug', 0.0069095112494849675), ('chest', 0.0069095112494849675), ('drug', 0.0069095112494849675), ('connection', 0.0069095112494849675), ('nothing', 0.0069095112494849675), ('rug', 0.0069095112494849675), ('went', 0.0069095112494849675), ('collided', 0.0069095112494849675), ('side', 0.0069095112494849675), ('fact', 0.0069095112494849675), ('left', 0.0069095112494849675), ('blow', 0.005100190039096408), ('right', 0.002550095019548204), ('love', 0.002550095019548204), ('finally', 0.002550095019548204), ('life', 0.002550095019548204), ('time', 0.002550095019548204), ('hey', 0.0), ('like', 0.0), ('always', 0.0), ('see', 0.0)]\n",
      "like 0.0\n",
      "always 0.0\n",
      "see 0.0\n",
      "hey 0.0\n",
      "[('tell', 0.039236153166718205), ('dance', 0.022420658952410404), ('fall', 0.022420658952410404), ('looking', 0.022420658952410404), ('milky', 0.022420658952410404), ('shooting', 0.022420658952410404), ('star', 0.022420658952410404), ('get', 0.0168154942143078), ('along', 0.0168154942143078), ('chance', 0.0168154942143078), ('light', 0.0168154942143078), ('atmosphere', 0.011210329476205202), ('sail', 0.011210329476205202), ('lights', 0.011210329476205202), ('permanent', 0.011210329476205202), ('heaven', 0.011210329476205202), ('across', 0.011210329476205202), ('overrated', 0.011210329476205202), ('reminds', 0.011210329476205202), ('best', 0.011210329476205202), ('sun', 0.011210329476205202), ('imagine', 0.011210329476205202), ('sweep', 0.011210329476205202), ('feet', 0.011210329476205202), ('head', 0.011210329476205202), ('faded', 0.011210329476205202), ('scar', 0.011210329476205202), ('make', 0.011210329476205202), ('afraid', 0.011210329476205202), ('without', 0.011210329476205202), ('wind', 0.011210329476205202), ('back', 0.01034349765582052), ('way', 0.01034349765582052), ('finally', 0.006206098593492312), ('day', 0.006206098593492312), ('miss', 0.006206098593492312), ('summer', 0.005605164738102601), ('toward', 0.005605164738102601), ('talks', 0.005605164738102601), ('chicken', 0.005605164738102601), ('find', 0.005605164738102601), ('pride', 0.005605164738102601), ('might', 0.005605164738102601), ('lonely', 0.005605164738102601), ('return', 0.005605164738102601), ('five', 0.005605164738102601), ('venus', 0.005605164738102601), ('spring', 0.005605164738102601), ('freeze', 0.005605164738102601), ('jupiter', 0.005605164738102601), ('dried', 0.005605164738102601), ('latte', 0.005605164738102601), ('constellation', 0.005605164738102601), ('even', 0.005605164738102601), ('since', 0.005605164738102601), ('moon', 0.005605164738102601), ('everything', 0.005605164738102601), ('ever', 0.005605164738102601), ('checks', 0.005605164738102601), ('told', 0.005605164738102601), ('mmm', 0.005605164738102601), ('wanted', 0.005605164738102601), ('change', 0.005605164738102601), ('fried', 0.005605164738102601), ('tracing', 0.005605164738102601), ('wrong', 0.005605164738102601), ('conversation', 0.005605164738102601), ('think', 0.005605164738102601), ('first', 0.005605164738102601), ('sticking', 0.005605164738102601), ('soy', 0.005605164738102601), ('story', 0.005605164738102601), ('jane', 0.005605164738102601), ('friend', 0.005605164738102601), ('fly', 0.005605164738102601), ('room', 0.005605164738102601), ('hour', 0.005605164738102601), ('plain', 0.005605164738102601), ('drops', 0.005605164738102601), ('deep', 0.005605164738102601), ('tae', 0.005605164738102601), ('walks', 0.005605164738102601), ('listens', 0.005605164738102601), ('acts', 0.005605164738102601), ('romance', 0.005605164738102601), ('vacation', 0.005605164738102601), ('june', 0.005605164738102601), ('rain', 0.005605164738102601), ('stay', 0.005605164738102601), ('grow', 0.005605164738102601), ('man', 0.005605164738102601), ('land', 0.005605164738102601), ('mozart', 0.005605164738102601), ('one', 0.004137399062328208), ('blow', 0.002068699531164104), ('know', 0.002068699531164104), ('hair', 0.002068699531164104), ('never', 0.002068699531164104), ('love', 0.002068699531164104), ('yeah', 0.002068699531164104), ('phone', 0.002068699531164104), ('soul', 0.002068699531164104), ('mind', 0.002068699531164104), ('time', 0.002068699531164104), ('like', 0.0), ('always', 0.0), ('see', 0.0), ('hey', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def freq(word, doc):\n",
    "    return doc.count(word)\n",
    " \n",
    " \n",
    "def word_count(doc):\n",
    "    return len(doc)\n",
    " \n",
    " \n",
    "def tf(word, doc):\n",
    "    return (freq(word, doc) / float(word_count(doc)))\n",
    " \n",
    " \n",
    "def num_docs_containing(word, list_of_docs):\n",
    "    count = 0\n",
    "    for document in list_of_docs:\n",
    "        if freq(word, document) > 0:\n",
    "            count += 1\n",
    "    return 1 + count\n",
    " \n",
    " \n",
    "def idf(word, list_of_docs):\n",
    "    return math.log(len(list_of_docs) /\n",
    "            float(num_docs_containing(word, list_of_docs)))\n",
    " \n",
    " \n",
    "def tf_idf(word, doc, list_of_docs):\n",
    "    return (tf(word, doc) * idf(word, list_of_docs))\n",
    "\n",
    "lyricsdoc=['dropsofjupiter','heysoulsister','meetvirginia']\n",
    "lyrics=[]\n",
    "for doc in lyricsdoc:\n",
    "    d=doc+\".txt\"\n",
    "    with open (d, \"r\") as myfile:\n",
    "        data=myfile.read().replace('\\n', ' ')\n",
    "    lyrics.append(data)\n",
    "\n",
    "\n",
    "idf_words = []\n",
    "tf = {}\n",
    "for i,l in enumerate(lyrics):\n",
    "    token = tokenizer.tokenize(l.lower())\n",
    "    token = [t for t in token if t not in stopwords and len(t)>2]\n",
    "    total=len(token)\n",
    "    tf_count = Counter(token)\n",
    "    tf_count={s:float(tf_count[s])/total for s in tf_count.keys()}\n",
    "    #print total\n",
    "    tf[lyricsdoc[i]] = tf_count\n",
    "        \n",
    "    idf_words.extend([word for word in set(token)])\n",
    "    \n",
    "idf_count = Counter(idf_words)\n",
    "\n",
    "tfidf = {}\n",
    "for song in tf.keys():\n",
    "    tfidf[song] = {}\n",
    "    for word in tf[song].keys():\n",
    "        tfidf[song][word] = tf[song][word] * math.log(float(3)/idf_count[word])\n",
    "        \n",
    "        if math.log(float(3)/idf_count[word])==0:\n",
    "            print word, math.log(float(3)/idf_count[word])\n",
    "            \n",
    "    sorted_tfidf = sorted(tfidf[song].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print sorted_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
